<ath_task
    id="generate-unit-tests"
    order="3"
    label="Generate Unit Tests"
    icon="TestTube2"
    tooltip="Generate unit tests for the selected files"
    requires="selected">

<ath_task_variant
    id="default"
    label="Default"
    tooltip="Default unit test generation">
# Target files

{{selected_files_with_info}}

# Task

Generate comprehensive unit tests for the target file(s) specified above. Your goal is to create effective tests that ensure code correctness, improve maintainability, and follow established best practices and project conventions. If unit tests already exist, your goal is to update and extend them.

## I. General Unit Testing Best Practices:

When generating tests, please adhere to these universal principles:

1.  **Isolation**:
    * Focus on testing individual units (functions, methods, classes, modules) in isolation from other parts of the system.
    * Mock or stub external dependencies (e.g., other modules, services, network requests, file system interactions, database calls) to ensure the test focuses solely on the behavior of the unit under test.

2.  **Coverage**:
    * Aim for thorough test coverage that instills confidence in the unit's correctness. This includes:
        * **Happy Paths**: Test the typical, expected behavior with valid inputs.
        * **Edge Cases & Boundary Conditions**: Test with inputs at the extremes of valid ranges, empty inputs, nulls, and other unusual but permissible inputs.
        * **Error Handling & Invalid Inputs**: Verify that the code handles errors gracefully, throws appropriate exceptions or returns error codes when given invalid or unexpected inputs.

3.  **Clarity & Readability (DAMP - Descriptive and Meaningful Phrases)**:
    * Write tests that are easy to understand and maintain. Test code is production code.
    * Use descriptive names for test suites (e.g., `describe` blocks) and individual test cases (e.g., `it` or `test` blocks) that clearly convey their purpose and the scenario being tested.
    * The test structure should be clear: Arrange (set up test conditions), Act (execute the unit under test), Assert (verify the outcome).

4.  **Repeatability & Reliability (FIRST - Fast, Independent, Repeatable, Self-Validating, Timely)**:
    * Tests must be deterministic, producing the same results every time they are run, regardless of the environment or the order of execution.
    * Avoid tests that rely on mutable external state, current time, or other volatile conditions unless specifically testing those aspects with proper controls.
    * Each test should be independent and not rely on the state or outcome of other tests.

5.  **Speed**:
    * Unit tests should execute quickly. Fast tests encourage frequent execution, providing rapid feedback to developers.

## II. Project-Specific Instructions & Conventions:

It is crucial that the generated tests integrate seamlessly with the existing project.

1.  **Identify Language, Frameworks, and Libraries**:
    * Automatically detect the programming language of the selected files.
    * Identify any testing frameworks (e.g., Jest, Mocha, PyTest, JUnit, NUnit, RSpec), assertion libraries (e.g., Chai, AssertJ), and mocking libraries (e.g., Jest mocks, Moq, Sinon.JS) already in use within the project.

2.  **Follow Existing Conventions**:
    * **Consistency is Key**: Adhere strictly to the project's established testing patterns, styles, and practices.
    * **Naming Conventions**: Follow existing naming conventions for test files (e.g., `*.test.ts`, `*.spec.js`, `test_*.py`), test suites, and individual test cases.
    * **Directory Structure**: Place new test files in the conventional location for the project (e.g., alongside source files in a `__tests__` subfolder, in a separate `tests/` or `src/tests` directory mirroring the source structure). Configuration files for testing frameworks (e.g., `jest.config.js`, `vitest.config.ts`, `pytest.ini`, `karma.conf.js`) often define test file locations, patterns, or root directories; check these for guidance if present.
    * **Existing Tests as a Guide**: Use existing unit tests in the project as the primary reference for style, structure, and common helper utilities.
    * **Project Documentation**: If present, refer to the content provided in `project_info` or other project documentation for any specific guidelines on testing.

3.  **Mocking & Stubbing**:
    * Employ appropriate mocking and stubbing techniques idiomatic to the identified testing framework.
    * Mock dependencies effectively to ensure tests are true unit tests, focusing on the isolated behavior of the code unit.

4.  **Assertions**:
    * Use the assertion style and library idiomatic to the identified framework.

5.  **Code Style**:
    * Ensure the generated test code adheres to the general coding style, formatting (e.g., as enforced by linters or formatters like ESLint, Prettier, Black, RuboCop), and conventions of the project.

## III. Handling Existing Unit Tests:

If unit tests already exist for the selected file(s), the primary goal is to ensure the test suite remains comprehensive, up-to-date, and aligned with the current implementation.

1.  **Analyze Existing Tests**:
    * Carefully review the existing tests to understand their current coverage, structure, and intent.
    * Identify which parts of the source code are already tested and how.

2.  **Align with Implementation**:
    * Verify that existing tests accurately reflect the current behavior of the code.
    * Update tests (e.g., assertions, mock setups, inputs) that have become outdated due to changes in the source code.
    * Ensure that existing tests still adhere to the best practices (Section I) and project conventions (Section II).

3.  **Extend Coverage**:
    * Identify any new functionalities, code paths, edge cases, or error conditions in the source code that are not covered by existing tests.
    * Generate additional test cases to address these gaps, aiming for thorough coverage as described in Section I.2. New tests should seamlessly integrate with the style and structure of existing ones.

4.  **Prune Obsolete Tests**:
    * Identify any test cases that are no longer relevant (e.g., testing functionality that has been removed or significantly refactored such that the original test's intent is void).
    * Propose the removal or modification of such obsolete tests, clearly stating the reason.

## IV. Action Points for AI:

1.  **Analyze**: Thoroughly analyze the selected files to understand their public API, core logic, dependencies, inputs, and outputs. If existing tests are present, analyze them as well (as per Section III.1).
2.  **Design Tests**:
    * If no tests exist: Devise a set of test cases that cover the aspects mentioned in "General Unit Testing Best Practices" and are relevant to the specific functionality of the target files.
    * If tests exist: Identify necessary updates, extensions, and potential pruning as outlined in "Handling Existing Unit Tests." Design new test cases to fill coverage gaps.
3.  **Generate/Modify Code**: Write the test code (for new tests) or suggest modifications (for existing tests) using the Athanor XML commands for file creation or modification.
4.  **Focus**: Prioritize tests and modifications that provide the most value in terms of verifying critical functionality, covering recent changes, and preventing regressions.
</ath_task_variant>
</ath_task>