# TODO

- Add branch/commits awareness (see https://chatgpt.com/share/67908472-3674-8006-88f4-824147f7ae48)

# RESOURCES

- https://www.reddit.com/r/LocalLLaMA/comments/1ftjbz3/shockingly_good_superintelligent_summarization/

---

The LLM (Large Language Model) module in the Athanor project is responsible for interactions with various AI providers. It's structured with a main service (`LLMServiceMain`) orchestrating requests, client adapters for provider-specific communication (`OpenAIClientAdapter`, `AnthropicClientAdapter`), common type definitions, and configuration. Here's an analysis of the module with suggestions for enhancing efficiency and maintainability:

## Current Strengths of the LLM Module

Before diving into suggestions, it's important to acknowledge aspects that are already well-implemented:

1.  **Centralized Configuration (`electron/modules/llm/main/config.ts`):**
    * This file excellently centralizes definitions for `SUPPORTED_PROVIDERS`, `SUPPORTED_MODELS`, default `LLMSettings` (`DEFAULT_LLM_SETTINGS`), and provider/model-specific overrides (`PROVIDER_DEFAULT_SETTINGS`, `MODEL_DEFAULT_SETTINGS`).
    * Utility functions like `getProviderById`, `getModelById`, `getDefaultSettingsForModel`, and `validateLLMSettings` promote consistency and reduce lookup logic elsewhere.

2.  **Clear Type Definitions (`electron/modules/llm/common/types.ts` and `electron/modules/llm/main/clients/types.ts`):**
    * Shared types (`LLMChatRequest`, `LLMResponse`, `LLMFailureResponse`, `ModelInfo`, `ProviderInfo`, etc.) establish a clear data contract across the module.
    * The `ILLMClientAdapter` interface and `InternalLLMChatRequest` (which ensures `settings` is `Required<LLMSettings>`) provide a strong contract for client adapters.
    * `ADAPTER_ERROR_CODES` offers a standardized set of error identifiers.

3.  **Robust `LLMServiceMain` Orchestration (`electron/modules/llm/main/LLMServiceMain.ts`):**
    * Effectively manages client adapters and integrates with `ApiKeyServiceMain` for secure API key handling.
    * Performs comprehensive request validation (provider, model, message structure, settings).
    * The logic for merging settings (`mergeSettingsForModel`) correctly applies overrides: request settings > model-specific > provider-specific > global defaults.

4.  **Effective `MockClientAdapter` (`electron/modules/llm/main/clients/MockClientAdapter.ts`):**
    * Provides a valuable tool for development and testing without making real API calls, capable of simulating various responses and error conditions.

5.  **Separation of Concerns (Main/Renderer):**
    * `LLMServiceMain.ts` handles backend logic in the main process.
    * `LLMServiceRenderer.ts` provides a clean IPC interface for the renderer process.
    * IPC channels are clearly defined in `LLM_IPC_CHANNELS` (`electron/modules/llm/common/types.ts`).

## Suggestions for Improvement

Here are some areas where the LLM module could be made more efficient and easier to maintain:

### 1. Refactor Client Adapter Error Handling

**Observation:**
The `createErrorResponse` methods within `OpenAIClientAdapter.ts` and `AnthropicClientAdapter.ts` contain similar logic for mapping HTTP status codes from provider SDK errors (like `OpenAI.APIError` or `Anthropic.APIError`) to the standardized `AdapterErrorCode` and `LLMFailureResponse`.

```typescript
// Snippet from OpenAIClientAdapter.ts - createErrorResponse
// Similar structure exists in AnthropicClientAdapter.ts
if (error instanceof OpenAI.APIError) {
  status = error.status;
  errorMessage = error.message;

  switch (error.status) {
    case 401:
      errorCode = ADAPTER_ERROR_CODES.INVALID_API_KEY;
      errorType = 'authentication_error';
      break;
    case 429:
      errorCode = ADAPTER_ERROR_CODES.RATE_LIMIT_EXCEEDED;
      errorType = 'rate_limit_error';
      break;
    // ... other cases ...
  }
} else if (error.code === 'ENOTFOUND' || error.code === 'ECONNREFUSED') {
  // ... network error handling ...
}
```

**Suggestion:**
Introduce a shared utility function or a base class method to handle common error mapping. This function could take the error object and an optional provider-specific checking function as arguments.

* **Create `electron/modules/llm/main/clients/adapterUtils.ts` (or similar):**
    This file could house a function like `mapProviderErrorToAdapterDetails`. This function would handle generic error types (e.g., network errors) and common HTTP status codes (401, 403, 404, 429, 5xx).
* **Adapters Call the Utility:** Each adapter's `createErrorResponse` would call this utility. For errors requiring provider-specific interpretation (e.g., specific messages within a 400 error indicating context length exceeded vs. content filter), the adapter can pass a callback to the utility or handle it after the common mapping.

**Benefit:** Reduces code duplication in error handling logic across adapters, making it easier to update error mappings globally and ensuring consistency.

### 2. Centralize Supported Model Information

**Observation:**
The `getAdapterInfo()` method in both `OpenAIClientAdapter.ts` and `AnthropicClientAdapter.ts` includes a `supportedModels` array:

```typescript
// OpenAIClientAdapter.ts
getAdapterInfo() {
  return {
    providerId: 'openai' as const,
    name: 'OpenAI Client Adapter',
    version: '1.0.0',
    supportedModels: [ /* model list */ ]
  };
}

// AnthropicClientAdapter.ts - similar structure
```

This list of supported models is also (and more comprehensively) defined in `electron/modules/llm/main/config.ts` under `SUPPORTED_MODELS`. This is a duplication of information.

**Suggestion:**
Make `electron/modules/llm/main/config.ts` the single source of truth for supported models.

* Remove the `supportedModels` array from `getAdapterInfo()` in individual client adapters.
* If this information is genuinely needed *from* an adapter instance elsewhere (which seems unlikely given `LLMServiceMain`'s validation role), `LLMServiceMain` could dynamically inject it when providing adapter info, using `getModelsByProvider(providerId)` from `config.ts`.
* The primary validation of whether a model is supported for a provider already happens in `LLMServiceMain.sendMessage` using `isModelSupported` which consults `config.ts`.

**Benefit:** Ensures that model support information is maintained in only one place, reducing the risk of inconsistencies and simplifying updates.

### 3. Dynamic Client Adapter Registration in `LLMServiceMain`

**Observation:**
Client adapters are currently instantiated and registered manually in the `LLMServiceMain` constructor:

```typescript
// LLMServiceMain.ts constructor
this.openAIClientAdapter = new OpenAIClientAdapter();
this.anthropicClientAdapter = new AnthropicClientAdapter();

this.registerClientAdapter('openai', this.openAIClientAdapter);
this.registerClientAdapter('anthropic', this.anthropicClientAdapter);
console.log('LLMServiceMain initialized with OpenAI and Anthropic adapters');
```

While manageable for a few adapters, this can become cumbersome as more providers are added.

**Suggestion:**
Implement a more dynamic, configuration-driven approach for adapter registration.

* Define a mapping of `ApiProviderId` to adapter constructors, potentially in `config.ts` or a new `adapterRegistry.ts`.
    ```typescript
    // Example for a new registry file or config.ts
    import { OpenAIClientAdapter } from './clients/OpenAIClientAdapter';
    import { AnthropicClientAdapter } from './clients/AnthropicClientAdapter';
    // Import other adapters as they are created

    export const ADAPTER_CONSTRUCTORS: Record<string, new (config?: any) => ILLMClientAdapter> = {
        'openai': OpenAIClientAdapter,
        'anthropic': AnthropicClientAdapter,
        // 'gemini': GeminiClientAdapter, // Future adapter
    };
    ```
* In the `LLMServiceMain` constructor, iterate over `SUPPORTED_PROVIDERS` (from `config.ts`) and instantiate/register adapters if a corresponding constructor exists in `ADAPTER_CONSTRUCTORS`.
    ```typescript
    // LLMServiceMain.ts constructor (simplified)
    this.clientAdapters = new Map();
    this.mockClientAdapter = new MockClientAdapter(); // Keep mock as a fallback

    let registeredCount = 0;
    for (const provider of SUPPORTED_PROVIDERS) {
        const AdapterClass = ADAPTER_CONSTRUCTORS[provider.id];
        if (AdapterClass) {
            // Pass baseURL or other adapter-specific config if needed, e.g., from a separate config structure
            const adapterConfig = {/* ... */};
            this.registerClientAdapter(provider.id, new AdapterClass(adapterConfig));
            registeredCount++;
        } else {
            console.warn(`No adapter constructor found for supported provider: ${provider.id}`);
        }
    }
    console.log(`LLMServiceMain initialized with ${registeredCount} adapters.`);
    ```

**Benefit:** Simplifies adding new providers. `LLMServiceMain` becomes less coupled to specific adapter implementations. Updates primarily involve adding the new adapter class, its entry in `SUPPORTED_PROVIDERS`/`SUPPORTED_MODELS` in `config.ts`, and its constructor to the `ADAPTER_CONSTRUCTORS` map.

### 4. Clarify System Message Handling (Documentation)

**Observation:**
The `LLMChatRequest` allows a top-level `systemMessage?: string;` and also messages with `role: 'system'` within the `messages` array. Different providers (like Anthropic) have specific ways they prefer system messages. `AnthropicClientAdapter` currently merges them:

```typescript
// AnthropicClientAdapter.ts - formatMessagesForAnthropic
let systemMessage = request.systemMessage;
// ...
for (const message of request.messages) {
  if (message.role === 'system') {
    if (systemMessage) {
      systemMessage += '\n\n' + message.content;
    } else {
      systemMessage = message.content;
    }
  }
}
```
This behavior is reasonable but might not be immediately obvious.

**Suggestion:**
Add clear JSDoc comments to the `LLMChatRequest` interface in `electron/modules/llm/common/types.ts` and/or the `sendMessage` method in `ILLMClientAdapter` (`electron/modules/llm/main/clients/types.ts`) to explain:
* How the top-level `systemMessage` and in-array `system` messages should ideally be handled or prioritized by adapters.
* The expected behavior if both are provided (e.g., merged, top-level takes precedence, etc.).

**Benefit:** Improves clarity for developers implementing new client adapters or using the `LLMServiceMain`.

### 5. Base URL Configuration for Adapters

**Observation:**
Both `OpenAIClientAdapter` and `AnthropicClientAdapter` accept an optional `baseURL` in their constructors:
```typescript
constructor(config?: { baseURL?: string }) {
  this.baseURL = config?.baseURL;
}
```
This is a good feature for flexibility (e.g., using proxies or OpenAI-compatible alternative APIs).

**Suggestion:**
If not already planned, ensure that this `baseURL` can be configured from a central application settings mechanism, allowing users or administrators to specify custom endpoints for each provider if needed. This is more of an integration point with a settings service rather than a direct refactor of the LLM module itself, but important for the usability of this feature. If dynamic adapter registration (Suggestion 3) is implemented, the adapter configuration (including `baseURL`) could be sourced from such a settings service.

**Benefit:** Enhances the flexibility of the application to work with different API endpoints.

## Conclusion

The LLM module in Athanor has a solid foundation, particularly in its configuration management and type safety. The suggestions above aim to reduce code duplication (especially in error handling), further centralize information (model support), improve scalability for adding new providers, and enhance clarity. By addressing these areas, the module can become even more efficient to maintain and extend in the future.